# Charger les bibliothèques nécessaires
library(rvest)
library(dplyr)
# Fonction pour extraire les données de chaque ligne
extract_row <- function(row) {
cells <- row %>%
html_nodes("td")
no <- cells[1] %>%
html_text() %>%
trimws()
title <- cells[2] %>%
html_text() %>%
trimws()
author <- cells[3] %>%
html_text() %>%
trimws()
publication <- cells[4] %>%
html_text() %>%
trimws()
date <- cells[5] %>%
html_text() %>%
trimws()
data.frame(
No = no,
Title = title,
Author = author,
Publication = publication,
Date = date,
stringsAsFactors = FALSE
)
}
# URL de la page contenant le tableau
base_url <- "https://guides.loc.gov/federalist-papers/full-text"
page <- read_html(base_url)
# Extraire le tableau avec la classe "table table-bordered"
table <- page %>%
html_node("table.table-bordered")
# Extraire les lignes du corps du tableau
table_body <- table %>%
html_node("tbody") %>%
html_nodes("tr")
# Appliquer la fonction d'extraction à chaque ligne et stocker les données
table_data <- lapply(table_body, extract_row)
# Concaténer les données en un seul data frame
final_data <- bind_rows(table_data)
# Afficher les premières lignes du tableau
head(final_data)
library(rvest)
# URL de la page
url <- "https://guides.loc.gov/federalist-papers/full-text"
# Lire le contenu de la page
page <- read_html(url)
# Extraire les balises <a> avec les attributs title et href
links <- page %>%
html_nodes("li a") %>%
html_attr("href") %>%
na.omit()  # Pour supprimer les valeurs NA
# Afficher les liens récupérés
print(links)
# URL de la page contenant la liste
url <- "https://guides.loc.gov/federalist-papers/full-text"
# Lire le contenu de la page
page <- read_html(url)
# Extraire la balise <ul> avec la classe spécifique
ul_node <- page %>%
html_node("ul.s-lg-subtab-ul.nav.nav-pills.nav-stacked")
# Extraire toutes les balises <li> dans cette <ul>
li_nodes <- ul_node %>%
html_nodes("li")
# Initialiser une liste pour stocker les résultats
results <- list()
# Parcourir chaque <li> et extraire les attributs title et href des balises <a>
for (li in li_nodes) {
a_node <- li %>%
html_node("a")
title <- a_node %>%
html_attr("title")
href <- a_node %>%
html_attr("href")
# Stocker les résultats dans la liste
results <- append(results, list(list(title = title, href = href)))
}
# Convertir les résultats en un data frame pour une manipulation plus facile
results_df <- do.call(rbind.data.frame, results)
# Afficher les résultats
print(results_df)
# Charger les bibliothèques nécessaires
library(rvest)
library(dplyr)
# URL de la page contenant la liste des Federalist Papers
url <- "https://guides.loc.gov/federalist-papers/full-text"
# Lire le contenu de la page
page <- read_html(url)
# Extraire la balise <ul> avec la classe spécifique
ul_node <- page %>%
html_node("ul.s-lg-subtab-ul.nav.nav-pills.nav-stacked")
# Extraire toutes les balises <li> dans cette <ul>
li_nodes <- ul_node %>%
html_nodes("li")
# Initialiser une liste pour stocker les résultats
links <- list()
# Parcourir chaque <li> et extraire les attributs title et href des balises <a>
for (li in li_nodes) {
a_node <- li %>%
html_node("a")
title <- a_node %>%
html_attr("title")
href <- a_node %>%
html_attr("href")
# Stocker les résultats dans la liste
links <- append(links, list(list(title = title, href = href)))
}
# Initialiser une liste pour stocker le texte complet de chaque lien
all_texts <- list()
# Fonction pour extraire le texte complet d'une page
extract_text <- function(link) {
page <- read_html(link)
text <- page %>%
html_node("div#s-lg-col-1.col-md-12") %>%
html_text(trim = TRUE)
return(text)
}
# Parcourir chaque lien et extraire le texte
for (i in 1:length(links)) {
link <- links[[i]]$href
# Ajouter le préfixe seulement si le lien ne contient pas déjà le préfixe
if (!grepl("^https://", link)) {
full_link <- paste0("https://guides.loc.gov", link)
} else {
full_link <- link
}
text <- extract_text(full_link)
all_texts <- append(all_texts, list(list(title = links[[i]]$title, text = text)))
}
# Afficher les résultats sous forme de liste
for (i in 1:length(all_texts)) {
cat("\n---", all_texts[[i]]$title, "---\n")
cat(all_texts[[i]]$text, "\n\n")
}
# Fonction pour extraire les auteurs à partir d'une URL
extract_authors <- function(url) {
# Lire le contenu de la page
page <- read_html(url)
# Extraire les noms des auteurs
authors <- page %>%
html_nodes("p:contains('Author:')") %>%
html_text() %>%
trimws() %>%
gsub("Author:", "", .)  # Supprimer le texte "Author:"
# S'il n'y a pas de paragraphes avec "Author:", chercher dans les balises <strong>
if (length(authors) == 0) {
authors <- page %>%
html_nodes("p strong") %>%
html_text() %>%
trimws()
}
return(authors)
}
# Initialiser une liste pour stocker les résultats
results <- list()
# Parcourir chaque lien dans la colonne href et extraire les auteurs
for (i in 1:nrow(results_df)) {
url <- results_df$href[i]
cat("Extraction des auteurs pour le lien:", url, "\n")
authors <- extract_authors(url)
results[[url]] <- authors
}
# Afficher les résultats
for (url in names(results)) {
cat("\nAuteurs pour le lien:", url, "\n")
print(results[[url]])
}
# results_df est le data frame contenant les URLs dans la colonne href
for (i in 1:nrow(results_df)) {
url <- results_df$href[i]
cat("Extraction des auteurs pour le lien:", url, "\n")
authors <- extract_authors(url)
results[[url]] <- authors
}
# Initialiser des listes pour stocker les paragraphes pour chaque auteur
Alexander_Hamilton <- list()
James_Madison <- list()
John_Jay <- list()
Hamilton_and_Madison <- list()
# Fonction pour extraire les auteurs et les paragraphes associés à partir d'une URL
extract_authors_and_paragraphs <- function(url) {
# Lire le contenu de la page
page <- read_html(url)
# Extraire tous les paragraphes
paragraphs <- page %>%
html_nodes("p") %>%
html_text() %>%
trimws()
# Initialiser une liste pour stocker les auteurs et leurs paragraphes
author_paragraphs <- list()
# Variables pour stocker l'auteur courant et ses paragraphes
current_author <- NULL
current_paragraphs <- c()
# Parcourir chaque paragraphe
for (para in paragraphs) {
if (grepl("^Author: ", para)) {
# Si le paragraphe contient un auteur, sauvegarder l'auteur et ses paragraphes précédents
if (!is.null(current_author)) {
author_paragraphs[[current_author]] <- current_paragraphs
}
current_author <- gsub("^Author: ", "", para)
current_paragraphs <- c()
} else if (grepl("^<strong>", para) && grepl("</strong>$", para)) {
# Si le paragraphe contient un auteur dans une balise <strong>, sauvegarder l'auteur et ses          #paragraphes précédents
if (!is.null(current_author)) {
author_paragraphs[[current_author]] <- current_paragraphs
}
current_author <- gsub("<.*?>", "", para)
current_paragraphs <- c()
} else {
# Ajouter le paragraphe à l'auteur courant
current_paragraphs <- c(current_paragraphs, para)
}
}
# Sauvegarder le dernier auteur et ses paragraphes
if (!is.null(current_author)) {
author_paragraphs[[current_author]] <- current_paragraphs
}
return(author_paragraphs)
}
# Parcourir chaque lien dans la colonne href et extraire les auteurs et leurs paragraphes
for (i in 1:nrow(results_df)) {
url <- results_df$href[i]
cat("Extraction des auteurs et des paragraphes pour le lien:", url, "\n")
author_paragraphs <- extract_authors_and_paragraphs(url)
# Ajouter les paragraphes aux listes correspondantes
for (author in names(author_paragraphs)) {
if (author == "Alexander Hamilton") {
Alexander_Hamilton <- c(Alexander_Hamilton, author_paragraphs[[author]])
} else if (author == "James Madison") {
James_Madison <- c(James_Madison, author_paragraphs[[author]])
} else if (author == "John Jay") {
John_Jay <- c(John_Jay, author_paragraphs[[author]])
} else if (author == "Hamilton and Madison") {
Hamilton_and_Madison <- c(Hamilton_and_Madison, author_paragraphs[[author]])
}
}
}
# Fonction pour afficher les paragraphes d'un auteur donné
afficher_paragraphes_auteur <- function(auteur) {
if (auteur == "Alexander Hamilton") {
cat("\nParagraphes d'Alexander Hamilton:\n", paste(Alexander_Hamilton, collapse = "\n\n"))
} else if (auteur == "James Madison") {
cat("\nParagraphes de James Madison:\n", paste(James_Madison, collapse = "\n\n"))
} else if (auteur == "John Jay") {
cat("\nParagraphes de John Jay:\n", paste(John_Jay, collapse = "\n\n"))
} else if (auteur == "Hamilton and Madison") {
cat("\nParagraphes de Hamilton and Madison:\n", paste(Hamilton_and_Madison, collapse = "\n\n"))
} else {
cat("Auteur non trouvé.\n")
}
}
# Appeler la fonction pour afficher les paragraphes d'un auteur spécifique
afficher_paragraphes_auteur("Alexander Hamilton")
#install.packages("syuzhet")
#install.packages("tm")
#install.packages("wordcloud")
library(syuzhet)
library(tm)
library(wordcloud)
# Function pour  l'analyse de sentiment
analyze_sentiment <- function(text_vector) {
# Convert the list to a single text
text <- paste(text_vector, collapse = " ")
# Analyse sentiment
sentiment_scores <- get_nrc_sentiment(text)
return(sentiment_scores)
}
# Analyse de sentiment pour Alexander Hamilton
if (length(Alexander_Hamilton) > 0) {
sentiment_alexander_hamilton <- analyze_sentiment(Alexander_Hamilton)
cat("\nAnalyse des sentiments pour Alexander Hamilton:\n")
print(sentiment_alexander_hamilton)
}
# Analyse de sentiment pour auteurs
if (length(James_Madison) > 0) {
sentiment_james_madison <- analyze_sentiment(James_Madison)
cat("\nAnalyse des sentiments pour James Madison:\n")
print(sentiment_james_madison)
}
if (length(John_Jay) > 0) {
sentiment_john_jay <- analyze_sentiment(John_Jay)
cat("\nAnalyse des sentiments pour John Jay:\n")
print(sentiment_john_jay)
}
if (length(Hamilton_and_Madison) > 0) {
sentiment_hamilton_and_madison <- analyze_sentiment(Hamilton_and_Madison)
cat("\nAnalyse des sentiments pour Hamilton and Madison:\n")
print(sentiment_hamilton_and_madison)
}
sentiment_results <- data.frame(
Author = character(),
Anger = numeric(),
Anticipation = numeric(),
Disgust = numeric(),
Fear = numeric(),
Joy = numeric(),
Sadness = numeric(),
Surprise = numeric(),
Trust = numeric(),
Negative = numeric(),
Positive = numeric(),
stringsAsFactors = FALSE
)
# Analyser les sentiments pour chaque auteur et ajouter les résultats au data frame
if (length(Alexander_Hamilton) > 0) {
sentiment_alexander_hamilton <- analyze_sentiment(Alexander_Hamilton)
sentiment_results <- rbind(sentiment_results, c("Alexander Hamilton", as.numeric(sentiment_alexander_hamilton)))
}
if (length(James_Madison) > 0) {
sentiment_james_madison <- analyze_sentiment(James_Madison)
sentiment_results <- rbind(sentiment_results, c("James Madison", as.numeric(sentiment_james_madison)))
}
if (length(John_Jay) > 0) {
sentiment_john_jay <- analyze_sentiment(John_Jay)
sentiment_results <- rbind(sentiment_results, c("John Jay", as.numeric(sentiment_john_jay)))
}
if (length(Hamilton_and_Madison) > 0) {
sentiment_hamilton_and_madison <- analyze_sentiment(Hamilton_and_Madison)
sentiment_results <- rbind(sentiment_results, c("Hamilton and Madison", as.numeric(sentiment_hamilton_and_madison)))
}
# Convertir les colonnes numériques
sentiment_results[,-1] <- lapply(sentiment_results[,-1], as.numeric)
# Renommer les colonnes
colnames(sentiment_results) <- c("Author", "Anger", "Anticipation", "Disgust", "Fear", "Joy", "Sadness", "Surprise", "Trust", "Negative", "Positive")
# Afficher les résultats sous forme de tableau
print(sentiment_results)
library(rvest)
library(dplyr)
library(syuzhet)
library(tidyverse)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
# Fonction pour extraire les mots les plus fréquents
extract_frequent_words <- function(text_vector, top_n = 15) {
# Convertir la liste en un seul texte
text <- paste(text_vector, collapse = " ")
# Créer un corpus
docs <- Corpus(VectorSource(text))
# Nettoyer le texte
docs <- docs %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords("en")) %>%
tm_map(removeWords, c("the", "and", "that", "this", "with", "for", "are", "was", "which")) %>%
tm_map(stripWhitespace)
# Créer une matrice de termes
dtm <- TermDocumentMatrix(docs)
matrix <- as.matrix(dtm)
word_freqs <- sort(rowSums(matrix), decreasing = TRUE)
freq_words <- data.frame(word = names(word_freqs), freq = word_freqs)
return(head(freq_words, top_n))
}
# Fonction pour créer un nuage de mots
create_wordcloud <- function(freq_words, title) {
wordcloud(
words = freq_words$word,
freq = freq_words$freq,
min.freq = 1,
max.words = 100,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"),
scale = c(3, 0.5)
)
title(main = title)
}
# Extraire et afficher les mots les plus fréquents pour chaque auteur
if (length(Alexander_Hamilton) > 0) {
freq_words_alexander_hamilton <- extract_frequent_words(Alexander_Hamilton)
cat("\nMots les plus fréquents pour Alexander Hamilton:\n")
print(freq_words_alexander_hamilton)
create_wordcloud(freq_words_alexander_hamilton, "Alexander Hamilton")
}
if (length(James_Madison) > 0) {
freq_words_james_madison <- extract_frequent_words(James_Madison)
cat("\nMots les plus fréquents pour James Madison:\n")
print(freq_words_james_madison)
create_wordcloud(freq_words_james_madison, "James Madison")
}
if (length(John_Jay) > 0) {
freq_words_john_jay <- extract_frequent_words(John_Jay)
cat("\nMots les plus fréquents pour John Jay:\n")
print(freq_words_john_jay)
create_wordcloud(freq_words_john_jay, "John Jay")
}
